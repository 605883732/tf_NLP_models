{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_load import load_train_data,load_test_data,get_batch_data, load_de_vocab, load_en_vocab\n",
    "from hyperparams import Hyperparams as hp\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "from modules import embedding,positional_encoding, multihead_attention,feedforward,label_smoothing\n",
    "import os,codecs\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务：  德语->英语的翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 德语词典\n",
    "de2idx,idx2de = load_de_vocab()\n",
    "\n",
    "# 英语词典\n",
    "en2idx,idx2en = load_en_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = load_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Der Großteil der Erde ist Meerwasser </S> <PAD> <PAD> <PAD>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([idx2de[word] for word in X[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Most of the planet is ocean water </S> <PAD> <PAD>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([idx2en[word] for word in Y[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Sources,Targets = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sie war eine jährige Frau namens Alex </S> <PAD> <PAD>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([idx2de[word] for word in X[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sie war eine jährige Frau namens Alex'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sources[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She was a yearold woman named Alex'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9796"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Graph():\n",
    "    def __init__(self,is_training=True):\n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            if is_training:\n",
    "                self.x,self.y,self.num_batch = get_batch_data()\n",
    "            else:\n",
    "                self.x = tf.placeholder(tf.int32,shape=(None,hp.maxlen))\n",
    "                self.y = tf.placeholder(tf.int32,shape=(None,hp.maxlen))\n",
    "\n",
    "            # define decoder inputs\n",
    "            self.decoder_inputs = tf.concat((tf.ones_like(self.y[:,:1]) * 2,self.y[:,:-1]) ,-1) # 2代表<S>，是decoder的初始输入\n",
    "\n",
    "            de2idx,idx2de = load_de_vocab()\n",
    "            en2idx,idx2en = load_en_vocab()\n",
    "\n",
    "            with tf.variable_scope(\"encoder\"):\n",
    "                # Embedding\n",
    "                self.enc = embedding(self.x,\n",
    "                                     vocab_size=len(de2idx),\n",
    "                                     num_units = hp.hidden_units,\n",
    "                                     zero_pad=True, # 让padding一直是0\n",
    "                                     scale=True,\n",
    "                                     scope=\"enc_embed\")\n",
    "\n",
    "                ## Positional Encoding\n",
    "                if hp.sinusoid:\n",
    "                    self.enc += positional_encoding(self.x,\n",
    "                                                    num_units = hp.hidden_units,\n",
    "                                                    zero_pad = False,\n",
    "                                                    scale = False,\n",
    "                                                    scope='enc_pe')\n",
    "\n",
    "                else:\n",
    "                    self.enc += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]),0),[tf.shape(self.x)[0],1]),\n",
    "                                          vocab_size = hp.maxlen,\n",
    "                                          num_units = hp.hidden_units,\n",
    "                                          zero_pad = False,\n",
    "                                          scale = False,\n",
    "                                          scope = \"enc_pe\")\n",
    "\n",
    "                ##Drop out\n",
    "                self.enc = tf.layers.dropout(self.enc,rate = hp.dropout_rate,\n",
    "                                             training = tf.convert_to_tensor(is_training))\n",
    "\n",
    "                ## Blocks\n",
    "                for i in range(hp.num_blocks):\n",
    "                    with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                        ### MultiHead Attention\n",
    "                        self.enc = multihead_attention(queries = self.enc,\n",
    "                                                       keys = self.enc,\n",
    "                                                       num_units = hp.hidden_units,\n",
    "                                                       num_heads = hp.num_heads,\n",
    "                                                       dropout_rate = hp.dropout_rate,\n",
    "                                                       is_training = is_training,\n",
    "                                                       causality = False\n",
    "                                                       )\n",
    "                        self.enc = feedforward(self.enc,num_units = [4 * hp.hidden_units,hp.hidden_units])\n",
    "\n",
    "\n",
    "\n",
    "            with tf.variable_scope(\"decoder\"):\n",
    "                # Embedding\n",
    "                self.dec = embedding(self.decoder_inputs,\n",
    "                                     vocab_size=len(en2idx),\n",
    "                                     num_units = hp.hidden_units,\n",
    "                                     scale=True,\n",
    "                                     scope=\"dec_embed\")\n",
    "\n",
    "                ## Positional Encoding\n",
    "                if hp.sinusoid:\n",
    "                    self.dec += positional_encoding(self.decoder_inputs,\n",
    "                                                    vocab_size = hp.maxlen,\n",
    "                                                    num_units = hp.hidden_units,\n",
    "                                                    zero_pad = False,\n",
    "                                                    scale = False,\n",
    "                                                    scope = \"dec_pe\")\n",
    "                else:\n",
    "                    self.dec += embedding(\n",
    "                        tf.tile(tf.expand_dims(tf.range(tf.shape(self.decoder_inputs)[1]), 0), [tf.shape(self.decoder_inputs)[0], 1]),\n",
    "                        vocab_size=hp.maxlen,\n",
    "                        num_units=hp.hidden_units,\n",
    "                        zero_pad=False,\n",
    "                        scale=False,\n",
    "                        scope=\"dec_pe\")\n",
    "\n",
    "                # Dropout\n",
    "                self.dec = tf.layers.dropout(self.dec,\n",
    "                                            rate = hp.dropout_rate,\n",
    "                                            training = tf.convert_to_tensor(is_training))\n",
    "\n",
    "                ## Blocks\n",
    "                for i in range(hp.num_blocks):\n",
    "                    with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                        ## Multihead Attention ( self-attention)\n",
    "                        self.dec = multihead_attention(queries=self.dec,\n",
    "                                                       keys=self.dec,\n",
    "                                                       num_units=hp.hidden_units,\n",
    "                                                       num_heads=hp.num_heads,\n",
    "                                                       dropout_rate=hp.dropout_rate,\n",
    "                                                       is_training=is_training,\n",
    "                                                       causality=True,\n",
    "                                                       scope=\"self_attention\")\n",
    "\n",
    "                        ## Multihead Attention ( vanilla attention)\n",
    "                        self.dec = multihead_attention(queries=self.dec,\n",
    "                                                       keys=self.enc,\n",
    "                                                       num_units=hp.hidden_units,\n",
    "                                                       num_heads=hp.num_heads,\n",
    "                                                       dropout_rate=hp.dropout_rate,\n",
    "                                                       is_training=is_training,\n",
    "                                                       causality=False,\n",
    "                                                       scope=\"vanilla_attention\")\n",
    "\n",
    "                        ## Feed Forward\n",
    "                        self.dec = feedforward(self.dec, num_units=[4 * hp.hidden_units, hp.hidden_units])\n",
    "\n",
    "            # Final linear projection\n",
    "            self.logits = tf.layers.dense(self.dec,len(en2idx))\n",
    "            self.preds = tf.to_int32(tf.argmax(self.logits,dimension=-1))\n",
    "            self.istarget = tf.to_float(tf.not_equal(self.y,0))\n",
    "            self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds,self.y)) * self.istarget / (tf.reduce_sum(self.istarget)))\n",
    "\n",
    "            if is_training:\n",
    "                # Loss\n",
    "                # 将one_hot中的0改成了一个很小的数，1改成了一个比较接近于1的数。\n",
    "                self.y_smoothed = label_smoothing(tf.one_hot(self.y,depth=len(en2idx)))\n",
    "                self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits,labels=self.y_smoothed)\n",
    "                self.mean_loss = tf.reduce_sum(self.loss * self.istarget) / (tf.reduce_sum(self.istarget))\n",
    "\n",
    "                self.global_step = tf.Variable(0,name='global_step',trainable=False)\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate = hp.lr,beta1 = 0.9,beta2 = 0.98,epsilon = 1e-8)\n",
    "                self.train_op = self.optimizer.minimize(self.mean_loss,global_step = self.global_step)\n",
    "\n",
    "                tf.summary.scalar('mean_loss',self.mean_loss)\n",
    "                self.merged = tf.summary.merge_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
